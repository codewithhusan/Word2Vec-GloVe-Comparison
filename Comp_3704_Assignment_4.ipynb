{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b507a808-1c0c-4706-86eb-b647eac5b247",
   "metadata": {},
   "source": [
    "# Assignment 4: Word Embeddings for Shakespearean English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f31520f-a1f3-4114-8513-88f39133c25b",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "41295092-bb6a-4d23-9160-f14c4aa654a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "from string import punctuation\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "59f0567c-1083-4b46-afee-5e0d2a146d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the shakespeare plays into a single variable and then converting them into lowercase\n",
    "text = nltk.corpus.gutenberg.raw(['shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'shakespeare-caesar.txt']).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "65860657-8bf9-4784-a34b-bf004e506d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the text into sentences and sentences into words\n",
    "text_words = [ tokenize.word_tokenize(sent) for sent in tokenize.sent_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bbeddb97-98ed-4833-850c-37ea038719bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[', 'the', 'tragedie', 'of', 'hamlet', 'by', 'william', 'shakespeare', '1599', ']', 'actus', 'primus', '.'], ['scoena', 'prima', '.'], ['enter', 'barnardo', 'and', 'francisco', 'two', 'centinels', '.'], ['barnardo', '.'], ['who', \"'s\", 'there', '?']]\n"
     ]
    }
   ],
   "source": [
    "print(text_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6b59e38f-6a42-499e-adf1-21b3bafde75f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# making a list of all the stopwords and adding our own as well\n",
    "punctuations = list(punctuation)\n",
    "stop_words = stopwords.words('english')\n",
    "all_stopwords = punctuations + stop_words + [\"'s\", \"mr.\", \"miss.\", \"mrs.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "15308980-1cc3-404f-bf52-e8149160ce20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_words = [\n",
    "    [word for word in sent if word not in all_stopwords]\n",
    "    for sent in text_words\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "23772bef-c0d7-474c-b367-9a4809d08bc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tragedie', 'hamlet', 'william', 'shakespeare', '1599', 'actus', 'primus'], ['scoena', 'prima'], ['enter', 'barnardo', 'francisco', 'two', 'centinels'], ['barnardo'], []]\n"
     ]
    }
   ],
   "source": [
    "print(filtered_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "dfdc8d1b-c64d-4ae8-9e68-406539554b7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d2363821-a0bd-451d-b33f-438892bb038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "spell = Speller(fast = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3f39d924-a828-4894-b750-46b342fa7425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defining a lemmatizing function to lematize the sentences \n",
    "def lemmatizee(text):\n",
    "    return [spell(lemmatizer.lemmatize(word)) for word in text] \n",
    "\n",
    "lemmatized_text = [lemmatizee(text) for text in filtered_words if text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "62380b45-f310-4115-82d8-5da245c29484",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tragedies', 'hamlet', 'william', 'shakespeare', '1599', 'acts', 'primes'], ['scoena', 'prima'], ['enter', 'barnardo', 'francisco', 'two', 'centinels'], ['barnardo'], ['fran'], ['nay', 'answer', 'stand', 'unfold', 'self', 'bar'], ['long', 'like', 'king', 'fran'], ['barnardo'], ['bar'], ['fran']]\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_text[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8596c791-fd16-406d-83f9-fa679aedba6d",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b93f5446-c283-43e5-8a6e-94bda378cd88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "57af4f62-9ab0-4cde-a9b7-abe1faf6474e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing a CBOW model\n",
    "cbow_model = Word2Vec(\n",
    "    sentences=lemmatized_text,   \n",
    "    vector_size=300,                 # Size of word embeddings (dimensionality)\n",
    "    window=5,                        # Context window size\n",
    "    min_count=3,                     # Ignores words with total frequency lower than this                          \n",
    "    epochs=10,                   \n",
    "    workers=6                        # Use 4 CPU cores for parallelization\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9efc6acf-17f0-493a-9406-fe6b4d8a2687",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Most Frequent Words and Their Counts:\n",
      " word: 'd count: 585\n",
      " word: have count: 444\n",
      " word: ham count: 337\n",
      " word: thou count: 306\n",
      " word: lord count: 306\n",
      " word: shall count: 300\n",
      " word: come count: 283\n",
      " word: king count: 248\n",
      " word: cesar count: 231\n",
      " word: enter count: 230\n",
      " word: here count: 225\n",
      " word: let count: 220\n",
      " word: good count: 217\n",
      " word: mac count: 205\n",
      " word: thy count: 202\n",
      " word: like count: 200\n",
      " word: one count: 182\n",
      " word: make count: 181\n",
      " word: v count: 178\n",
      " word: thee count: 174\n"
     ]
    }
   ],
   "source": [
    "# Access the word vectors\n",
    "vocab = cbow_model.wv\n",
    "\n",
    "# Get the 20 most frequent words in the vocabulary along with their counts\n",
    "print(\"20 Most Frequent Words and Their Counts:\")\n",
    "for word in list(vocab.key_to_index)[:20]:\n",
    "    count = vocab.get_vecattr(word, \"count\")\n",
    "    print(f\" word: {word} count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1a67617e-ee5d-4c07-a005-54a9c18d5fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing a Skip gram model\n",
    "skip_gram_model = Word2Vec(\n",
    "    sentences=lemmatized_text,   \n",
    "    vector_size=300,\n",
    "    window=5,                        \n",
    "    min_count=3, \n",
    "    sg = 1,                                      \n",
    "    epochs=10,                   \n",
    "    workers=6                        \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6b8af19a-20ab-4438-af8e-6c3dd30f60a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\123hu\\anaconda3\\envs\\UL\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 300)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Importing the pre trained glove model and then converting it to vectors\n",
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# glove_input_file = 'glove.6B.300d.txt'\n",
    "# out_file = './glove.6B.300d.w2vformat.txt'\n",
    "\n",
    "# glove2word2vec(glove_input_file, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9a99225b-f77b-4b57-b576-c928860e8459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load GloVe 300-dimensional embeddings\n",
    "glove_model = KeyedVectors.load_word2vec_format(out_file, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d8541-0ad1-42c3-b3b2-745824f03a59",
   "metadata": {},
   "source": [
    "## Pretrained GloVe Model Background\n",
    "### The GloVe model from Stanford was trained on multiple large corpora, including:\n",
    "\n",
    "1. **Wikipedia (2014 edition)**: A comprehensive collection of Wikipedia articles.\n",
    "2. **Gigaword 5**: A massive collection of English news text, including newswire from sources like the Associated Press, New York Times, and others.\n",
    "3. **Common Crawl**: Over 840 billion tokens from a variety of web sources.\n",
    "4. **Twitter**: A dataset of 2 billion tweets to capture social media vocabulary.\n",
    "\n",
    "### The available GloVe models differ by vector size and dataset. The commonly used ones are:\n",
    "1. **6B**: 6 billion tokens (Wikipedia and Gigaword 5).\n",
    "2. **42B**: 42 billion tokens (Common Crawl).\n",
    "3. **840B**: 840 billion tokens (Common Crawl, uncased).\n",
    "4. **27B**: 27 billion tokens (Twitter).\n",
    "\n",
    "## Key Differences in GloVe Training and Word2Vec Training\n",
    "1. **Scale of Data**: GloVe was trained on vast datasets encompassing billions of tokens, while the Word2Vec models trained on Shakespeare's plays are limited to a specific vocabulary and style of language.\n",
    "2. **Contextual Breadth**: GloVe’s training data spans a wide range of topics, styles, and vocabulary, making it more generalizable, while the Shakespeare Word2Vec models are more tailored to the specific context of early modern English literature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d42520-d739-4890-ada6-067cbaaf5d13",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b546660d-f133-4051-a8c0-ab65e218bb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of models with names and access methods\n",
    "models = {\n",
    "    \"CBOW Word2Vec\": cbow_model.wv,\n",
    "    \"Skip-Gram Word2Vec\": skip_gram_model.wv,\n",
    "    \"GloVe\": glove_model\n",
    "}\n",
    "\n",
    "# Define the terms to compare\n",
    "terms = ['hamlet', 'cauldron', 'nature', 'spirit', 'general', 'prythee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bfd72163-f69d-43ee-9d63-4bcfc3c8c863",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                hamlet                                \n",
      "----------------------------------------------------------------------\n",
      "         Cbow_model     Skip_gram_model         Glove_model\n",
      "0   (queen, 0.9998)     (queen, 0.9895)   (village, 0.5545)\n",
      "1    (lady, 0.9997)   (laertes, 0.9862)      (town, 0.4956)\n",
      "2    (king, 0.9997)  (polonius, 0.9831)   (hamlets, 0.4852)\n",
      "3  (banquo, 0.9997)      (king, 0.9824)   (othello, 0.4451)\n",
      "4  (either, 0.9997)   (horatio, 0.9822)  (situated, 0.4446)\n",
      "                               cauldron                               \n",
      "----------------------------------------------------------------------\n",
      "         Cbow_model       Skip_gram_model          Glove_model\n",
      "0    (self, 0.9995)      (bubble, 0.9992)    (caldron, 0.6924)\n",
      "1      (th, 0.9995)       (crack, 0.9991)      (flame, 0.5382)\n",
      "2    (even, 0.9995)  (distracted, 0.9991)  (cauldrons, 0.4885)\n",
      "3   (cause, 0.9995)     (warning, 0.9991)      (torch, 0.4723)\n",
      "4  (within, 0.9995)     (current, 0.9991)        (lit, 0.4593)\n",
      "                                nature                                \n",
      "----------------------------------------------------------------------\n",
      "        Cbow_model  Skip_gram_model          Glove_model\n",
      "0   (till, 0.9999)  (whose, 0.9958)    (subject, 0.5311)\n",
      "1     (th, 0.9999)  (state, 0.9956)     (rather, 0.5253)\n",
      "2    (may, 0.9999)   (foul, 0.9953)    (aspects, 0.5226)\n",
      "3  (whose, 0.9999)  (seems, 0.9952)       (kind, 0.5172)\n",
      "4     (vp, 0.9999)  (forme, 0.9951)  (existence, 0.5149)\n",
      "                                spirit                                \n",
      "----------------------------------------------------------------------\n",
      "        Cbow_model   Skip_gram_model        Glove_model\n",
      "0   (must, 0.9999)  (dagger, 0.9963)  (spirits, 0.5449)\n",
      "1    (are, 0.9999)  (valour, 0.9962)      (faith, 0.53)\n",
      "2    (men, 0.9999)   (angel, 0.9961)  (courage, 0.5283)\n",
      "3  (whose, 0.9999)   (wound, 0.9961)    (sense, 0.5133)\n",
      "4   (even, 0.9999)     (brow, 0.996)  (passion, 0.5128)\n",
      "                               general                                \n",
      "----------------------------------------------------------------------\n",
      "       Cbow_model     Skip_gram_model          Glove_model\n",
      "0  (self, 0.9998)       (mou, 0.9988)       (gen., 0.6153)\n",
      "1    (th, 0.9998)  (ordinary, 0.9988)    (secretary, 0.55)\n",
      "2  (till, 0.9998)      (drop, 0.9988)  (brigadier, 0.4928)\n",
      "3  (fire, 0.9998)    (temper, 0.9988)        (chief, 0.49)\n",
      "4   (men, 0.9998)      (moon, 0.9988)    (command, 0.4739)\n",
      "<class 'KeyError'>\n",
      "                               prythee                                \n",
      "----------------------------------------------------------------------\n",
      "        Cbow_model   Skip_gram_model\n",
      "0    (ile, 0.9993)   (com'st, 0.999)\n",
      "1  (earth, 0.9993)     (dist, 0.999)\n",
      "2   (thee, 0.9993)     (fla., 0.999)\n",
      "3   (self, 0.9993)  (nobles, 0.9989)\n",
      "4     (vp, 0.9992)    (hilt, 0.9988)\n"
     ]
    }
   ],
   "source": [
    "# cheching for most similar terms in each model\n",
    "for i in terms:\n",
    "    results = {}\n",
    "    try:\n",
    "        results['Cbow_model'] = [(word, round(prob, 4)) for word, prob in cbow_model.wv.most_similar(i, topn=5)]\n",
    "        results['Skip_gram_model'] = [(word, round(prob, 4)) for word, prob in skip_gram_model.wv.most_similar(i, topn=5)]\n",
    "        results['Glove_model'] = [(word, round(prob, 4)) for word, prob in glove_model.most_similar(i, topn=5)]\n",
    "        \n",
    "    except KeyError:\n",
    "        print(KeyError)\n",
    "            \n",
    "    print(i.center(70))\n",
    "    print('-'*70)\n",
    "    print(pd.DataFrame(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b85b721-f0b1-4cdf-af89-59fcc166b0dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analysis and Comparison\n",
    "###                                                Hamlet\n",
    "\n",
    "- CBOW: Finds related characters and themes but includes some less relevant terms like \"sweet\" and \"wife.\"\n",
    "- Skip-gram: Captures Shakespearean characters directly associated with Hamlet (e.g., \"Laertes,\" \"Horatio\").\n",
    "- GloVe: Returns broader terms (e.g., \"village,\" \"town,\" \"Othello\"), which reflect general literary associations.\n",
    "\n",
    "###                                                Cauldron\n",
    "\n",
    "- CBOW: Mostly irrelevant terms like \"self\" and \"even,\" with limited relevance to cauldron imagery.\n",
    "- Skip-gram: Provides relevant imagery-rich words like \"bubble,\" \"crack,\" and \"flower,\" fitting the supernatural theme.\n",
    "- GloVe: Offers generally associated terms like \"flame\" and \"torch,\" focusing more on practical meanings than context.\n",
    "\n",
    "###                                                Nature\n",
    "\n",
    "- CBOW: Contains high-frequency words like \"till\" and \"whose,\" which don’t enhance the meaning.\n",
    "- Skip-gram: Identifies words like \"earth,\" \"breath,\" and \"seems,\" capturing natural and philosophical themes.\n",
    "- GloVe: Shows general terms like \"subject\" and \"existence,\" which lack specific Shakespearean associations.\n",
    "\n",
    "###                                                Spirit\n",
    "\n",
    "- CBOW: Includes frequent functional terms such as \"whose\" and \"are\" without context relevance.\n",
    "- Skip-gram: Finds thematically relevant terms like \"angel\" and \"damn,\" aligning with supernatural elements.\n",
    "- GloVe: Offers broad associations (e.g., \"spirits,\" \"faith,\" \"courage\") but lacks depth in supernatural nuances.\n",
    "\n",
    "###                                                General\n",
    "\n",
    "- CBOW: Returns generic terms like \"fire\" and \"self\" without strong contextual ties.\n",
    "- Skip-gram: Captures loosely related terms like \"drone\" and \"judge,\" somewhat relevant to character roles.\n",
    "- GloVe: Accurately reflects administrative and military meanings (e.g., \"gen.,\" \"brigadier,\" \"chief\").\n",
    "\n",
    "###                                                Prythee\n",
    "\n",
    "- CBOW: Contains mostly irrelevant terms such as \"ile\" and \"self.\"\n",
    "- Skip-gram: Provides loosely associated words but lacks strong connection (e.g., \"does,\" \"nobles\").\n",
    "- GloVe: Does not recognize this term due to its rarity outside of Shakespearean context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6d343d78-2247-40ec-bcb0-9b6f01051e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                       brutus:murder                        \n",
      "CBOW: [0.99658227] | skipgram: [0.9468968] | GLOVE: [0.06155146]\n",
      "\n",
      "                      macbeth:gertrude                      \n",
      "CBOW: [0.99893713] | skipgram: [0.9367072] | GLOVE: [0.23251496]\n",
      "\n",
      "                     fortinbras:norway                      \n",
      "CBOW: [0.9996064] | skipgram: [0.9988509] | GLOVE: [0.05642105]\n",
      "\n",
      "                        rome:norway                         \n",
      "CBOW: [0.9996088] | skipgram: [0.9952338] | GLOVE: [0.13604455]\n",
      "\n",
      "                        ghost:spirit                        \n",
      "CBOW: [0.99959236] | skipgram: [0.99067837] | GLOVE: [0.3149283]\n",
      "\n",
      "                       macbeth:hamlet                       \n",
      "CBOW: [0.9995727] | skipgram: [0.9406717] | GLOVE: [0.3862637]\n"
     ]
    }
   ],
   "source": [
    "# Getting the similarity score between given words among the three models\n",
    "word_pairs = [\n",
    "    ('brutus', 'murder'),\n",
    "    ('macbeth', 'gertrude'),\n",
    "    ('fortinbras', 'norway'),\n",
    "    ('rome', 'norway'),\n",
    "    ('ghost', 'spirit'),\n",
    "    ('macbeth', 'hamlet')\n",
    "]\n",
    "for word1, word2 in word_pairs:\n",
    "    try: \n",
    "        results_score = { \n",
    "            \"similarity_cbow\":cbow_model.wv.cosine_similarities(cbow_model.wv[word1], [cbow_model.wv[word2]]),\n",
    "            \"similarity_skip_gram\":skip_gram_model.wv.cosine_similarities(skip_gram_model.wv[word1], [skip_gram_model.wv[word2]]),\n",
    "            \"similarity_glove\": glove_model.cosine_similarities(glove_model[word1], [glove_model[word2]])\n",
    "            }\n",
    "\n",
    "    except(KeyError) as e:\n",
    "        print(e)\n",
    "    print()\n",
    "    print(f\"{word1}:{word2}\".center(60))\n",
    "    print(f\"CBOW: {results_score['similarity_cbow']} | skipgram: {results_score['similarity_skip_gram']} | GLOVE: {results_score['similarity_glove']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730aa2d5-c2c0-49c0-92fe-13949c7f316f",
   "metadata": {},
   "source": [
    "## General Summary\n",
    "### (‘brutus’, ‘murder’)\n",
    "\n",
    "- CBOW: 0.9969 - Shows a very high similarity, indicating that the model captures the strong thematic link between Brutus and the act of murder in Julius Caesar.\n",
    "- Skip-gram: 0.9569 - Also reflects a strong relationship, although slightly lower than CBOW. This still indicates an effective capture of the theme.\n",
    "- GloVe: 0.0616 - Very low similarity, suggesting GloVe fails to recognize the thematic connection, likely due to its more general training data.\n",
    "\n",
    "### (‘lady macbeth’, ‘queen gertrude’)\n",
    "\n",
    "- CBOW: 0.9989 - Extremely high similarity, effectively capturing the connection between these two prominent female characters in Shakespeare.\n",
    "- Skip-gram: 0.9330 - Also shows a strong association, although less than CBOW, which is still impressive for character relationships.\n",
    "- GloVe: 0.2325 - Low to moderate similarity, indicating some recognition of the characters’ royalty but lacking depth in their contextual significance.\n",
    "\n",
    "### (‘fortinbras’, ‘norway’)\n",
    "\n",
    "- CBOW: 0.9996 - Indicates a very strong relationship, successfully connecting Fortinbras with his association to Norway in Hamlet.\n",
    "- Skip-gram: 0.9987 - Also shows high similarity, further confirming the connection.\n",
    "- GloVe: 0.0564 - Very low, indicating a failure to establish the relationship likely due to lack of context in the training data.\n",
    "\n",
    "### (‘rome’, ‘norway’)\n",
    "\n",
    "- CBOW: 0.9997 - Shows an unexpectedly high similarity, but this may stem from word frequency rather than thematic connection.\n",
    "- Skip-gram: 0.9960 - Also indicates a high similarity, which might not accurately reflect their lack of direct relationship in Shakespearean context.\n",
    "- GloVe: 0.1360 - Low similarity, recognizing some association but not capturing their relationship effectively.\n",
    "\n",
    "### (‘ghost’, ‘spirit’)\n",
    "\n",
    "- CBOW: 0.9996 - Very high similarity, effectively capturing the connection between the ghost and the broader theme of spirits in Hamlet.\n",
    "- Skip-gram: 0.9887 - Also shows strong similarity, indicating a good recognition of the supernatural element.\n",
    "- GloVe: 0.3149 - Moderate similarity, indicating some recognition of the terms' overlap but lacking the depth of connection captured by the other models.\n",
    "\n",
    "### (‘macbeth’, ‘hamlet’)\n",
    "\n",
    "- CBOW: 0.9995 - Extremely high similarity, capturing the strong literary connection between these two central characters in Shakespeare's works.\n",
    "- Skip-gram: 0.9387 - Also indicates a good relationship, although slightly lower than CBOW.\n",
    "- GloVe: 0.3863 - Moderate similarity, indicating some recognition of the characters' importance but lacking the context specific to Shakespeare’s themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ec1306fc-0d22-4c5d-bdb9-06921eac7645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_list = ['denmark','queen', 'scotland', 'army', 'general', 'father', 'woman', 'mother', 'man']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4fb9f604-30de-49d9-b5a7-60580ebcbf2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          |Denmark + Queen|                           \n",
      "----------------------------------------------------------------------\n",
      "  similar_terms_cbow   similar_terms_skipgram similar_terms_glove\n",
      "0       (th, 0.9999)        (laertes, 0.9972)    (sweden, 0.6307)\n",
      "1     (wife, 0.9999)        (ophelia, 0.9971)    (norway, 0.5922)\n",
      "2     (poor, 0.9999)  (guildensterne, 0.9969)      (king, 0.5871)\n",
      "3   (nature, 0.9999)      (attendant, 0.9968)    (danish, 0.5676)\n",
      "4     (many, 0.9999)    (rosincrance, 0.9968)  (princess, 0.5676)\n"
     ]
    }
   ],
   "source": [
    "# getting the top 5 similar terms by adding two or three terms from three models\n",
    "try:\n",
    "    res = { \n",
    "            \"similar_terms_cbow\":[(word, round(prob, 4)) for word, prob in cbow_model.wv.most_similar(positive=['denmark', 'queen'], topn = 5)],\n",
    "            \"similar_terms_skipgram\":[(word, round(prob, 4)) for word, prob in skip_gram_model.wv.most_similar(positive=['denmark', 'queen'], topn = 5)],\n",
    "            \"similar_terms_glove\": [(word, round(prob, 4)) for word, prob in glove_model.most_similar(positive=['denmark', 'queen'], topn = 5)]\n",
    "            }     \n",
    "except(KeyError):\n",
    "    print(KeyError)   \n",
    "print(\"|Denmark + Queen|\".center(70))\n",
    "print('-' * 70)\n",
    "print(pd.DataFrame(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4e84066f-f1bb-4866-b129-5c1b06671d77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     |scotland + army + general|                      \n",
      "----------------------------------------------------------------------\n",
      "  similar_terms_cbow similar_terms_skipgram  similar_terms_glove\n",
      "0     (like, 0.9997)          (ran, 0.9995)   (military, 0.6035)\n",
      "1     (both, 0.9997)       (having, 0.9994)       (gen., 0.5679)\n",
      "2       (th, 0.9997)    (advantage, 0.9994)    (command, 0.5675)\n",
      "3   (though, 0.9997)     (speaking, 0.9994)  (commander, 0.5628)\n",
      "4      (way, 0.9997)       (statue, 0.9994)     (forces, 0.5607)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    res_2 = { \n",
    "            \"similar_terms_cbow\":[(word, round(prob, 4)) for word, prob in cbow_model.wv.most_similar(positive=['scotland', 'army', 'general'], topn = 5)],\n",
    "            \"similar_terms_skipgram\":[(word, round(prob, 4)) for word, prob in skip_gram_model.wv.most_similar(positive=['scotland', 'army', 'general'], topn = 5)],\n",
    "            \"similar_terms_glove\": [(word, round(prob, 4)) for word, prob in glove_model.most_similar(positive=['scotland', 'army', 'general'], topn = 5)]\n",
    "            }     \n",
    "except(KeyError):\n",
    "    print(KeyError)   \n",
    "print(\"|scotland + army + general|\".center(70))\n",
    "print('-' * 70)\n",
    "print(pd.DataFrame(res_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "59eb280e-87e8-4505-a4b1-b662e8e5a073",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        |father - man + woman|                        \n",
      "----------------------------------------------------------------------\n",
      "  similar_terms_cbow similar_terms_skipgram   similar_terms_glove\n",
      "0   (mother, 0.9996)       (hamlet, 0.9778)      (mother, 0.8266)\n",
      "1     (many, 0.9996)       (mother, 0.9774)    (daughter, 0.7898)\n",
      "2     (wife, 0.9996)      (laertes, 0.9753)     (husband, 0.7276)\n",
      "3   (heaven, 0.9996)           (oh, 0.9743)        (wife, 0.7243)\n",
      "4     (thus, 0.9996)         (soul, 0.9708)  (grandmother, 0.696)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    res_3 = { \n",
    "            \"similar_terms_cbow\":[(word, round(prob, 4)) for word, prob in cbow_model.wv.most_similar(positive=['father', 'woman'], negative=['man'], topn = 5)],\n",
    "            \"similar_terms_skipgram\":[(word, round(prob, 4)) for word, prob in skip_gram_model.wv.most_similar(positive=['father', 'woman'], negative=['man'], topn = 5)],\n",
    "            \"similar_terms_glove\": [(word, round(prob, 4)) for word, prob in glove_model.most_similar(positive=['father', 'woman'], negative=['man'], topn = 5)]\n",
    "            }     \n",
    "except(KeyError):\n",
    "    print(KeyError)   \n",
    "print(\"|father - man + woman|\".center(70))\n",
    "print('-' * 70)   \n",
    "print(pd.DataFrame(res_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "364a5c95-b227-4da2-be49-0c446258de7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        |mother - woman + man|                        \n",
      "----------------------------------------------------------------------\n",
      "  similar_terms_cbow similar_terms_skipgram    similar_terms_glove\n",
      "0     (hand, 0.9996)         (kill, 0.9897)       (father, 0.7923)\n",
      "1    (great, 0.9996)         (poor, 0.9895)      (brother, 0.7102)\n",
      "2  (purpose, 0.9996)        (since, 0.9883)          (son, 0.6802)\n",
      "3     (life, 0.9996)       (breath, 0.9883)        (uncle, 0.6393)\n",
      "4    (still, 0.9996)      (denmark, 0.9882)  (grandfather, 0.6236)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    res_4 = { \n",
    "            \"similar_terms_cbow\":[(word, round(prob, 4)) for word, prob in cbow_model.wv.most_similar(positive=['mother', 'man'], negative=['woman'], topn = 5)],\n",
    "            \"similar_terms_skipgram\":[(word, round(prob, 4)) for word, prob in skip_gram_model.wv.most_similar(positive=['mother', 'man'], negative=['woman'], topn = 5)],\n",
    "            \"similar_terms_glove\": [(word, round(prob, 4)) for word, prob in glove_model.most_similar(positive=['mother', 'man'], negative=['woman'], topn = 5)]\n",
    "            }     \n",
    "except(KeyError):\n",
    "    print(KeyError)   \n",
    "print(\"|mother - woman + man|\".center(70))\n",
    "print('-' * 70)   \n",
    "print(pd.DataFrame(res_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94da5b9-0d0c-4c86-83b6-33b32d7f5982",
   "metadata": {},
   "source": [
    "### Denmark + Queen\n",
    "- CBOW Model: Generates high-frequency but generic terms like \"th\" and \"wife,\" lacking specific context.\n",
    "- Skip-gram Model: Identifies key characters such as \"attendant\" and \"Ophelia,\" effectively capturing royal relationships.\n",
    "- GloVe Model: Produces broader terms like \"Sweden\" and \"king,\" but less relevant to the specific query.\n",
    "\n",
    "### Scotland + Army + General\n",
    "- CBOW Model: Returns generic terms such as \"like\" and \"way,\" showing limited context.\n",
    "- Skip-gram Model: Yields terms like \"legion,\" indicating some military relevance, but includes less relevant terms.\n",
    "- GloVe Model: Captures military terms like \"military\" and \"commander,\" demonstrating a better understanding of the query's themes.\n",
    "\n",
    "### Father - Man + Woman\n",
    "- CBOW Model: Produces terms like \"mother\" and \"wife,\" reflecting gender-related themes, but lacks specificity.\n",
    "- Skip-gram Model: Successfully identifies \"mother\" and \"soul,\" showing familial relationships but with some irrelevant terms.\n",
    "- GloVe Model: Effectively captures family roles with terms like \"mother\" and \"daughter,\" demonstrating a solid grasp of gender dynamics.\n",
    "\n",
    "### Mother - Woman + Man\n",
    "- CBOW Model: Generates generic terms like \"death\" and \"life,\" showing limited relevance to the query.\n",
    "- Skip-gram Model: Produces terms like \"kill\" and \"brother,\" indicating darker themes and familial connections.\n",
    "- GloVe Model: Identifies terms like \"father\" and \"son,\" successfully reflecting family dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b66d01-dd89-49d6-afa2-cbf2d8cfade9",
   "metadata": {},
   "source": [
    "### Overall Comments on Model Performance\n",
    "1. CBOW Model:\n",
    "\n",
    "- Strengths: Fast and efficient for training, producing embeddings for words based on context. It effectively captures some relationships and frequently returns common terms.\n",
    "- Weaknesses: Tends to generate generic or less relevant terms, especially in nuanced contexts. It may overlook important character relationships and themes in Shakespearean English.\n",
    "\n",
    "2. Skip-gram Model:\n",
    "\n",
    "- Strengths: Excels in capturing contextual nuances and relationships, particularly in more complex and thematic phrases. It identifies relevant character names and actions more effectively.\n",
    "- Weaknesses: Slower to train compared to CBOW, and can sometimes generate irrelevant terms when context is less clear.\n",
    "\n",
    "3. GloVe Model:\n",
    "\n",
    "- Strengths: Leverages global statistical information to produce embeddings that capture relationships effectively. It provides relevant terms in many contexts and is good at capturing similarities based on co-occurrence.\n",
    "- Weaknesses: May miss subtleties and context-specific meanings, particularly in dense literary language like that of Shakespeare. It can yield broader, less focused terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4597623e-c36a-4007-a7f1-cdab09eacb7f",
   "metadata": {},
   "source": [
    "### Recommendations for Training a Better Word Embedding Model\n",
    "1. Diverse Shakespearean Texts\n",
    "2. Contextual Literature\n",
    "3. Thematic Groupings\n",
    "4. Collaborative Data:\n",
    "5. Additional Language Resources\n",
    "6. Fine-Tuning with Modern Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de977f-dd5b-4752-8ca9-64492e6bd5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
